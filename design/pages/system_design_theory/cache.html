<div class="container">

        <div class="content">

            <h3 class="section-title">What is Cache?</h3>
            <p>Cache is a mechanism used to temporarily store frequently accessed data in a fast storage layer, so future requests for that data are served faster. By reducing the load on the primary data source (e.g., database or web service), caching improves system performance and scalability, particularly for read-heavy applications.</p>
            <img src="../../images/cache.PNG" alt="cache" style="display: block; margin: auto; width: 30%;">
            <br>
            <h5>Benefits of Caching (Pros)</h5>
            <ul>
              <li><b>Improved Performance:</b> Frequently requested data is served from the cache, leading to faster response times.</li>
              <li><b>Reduced Load on Primary Storage:</b> Caching minimizes the number of direct queries to the database or backend services.</li>
              <li><b>Cost-Efficient:</b> With fewer database calls, you save on infrastructure costs, especially in high-traffic environments.</li>
            </ul>
            <hr>
            <h5>Drawbacks of Caching (Cons)</h5>
            <ul>
              <li><b>Stale Data:</b> Cached data might become outdated if the primary data source changes, leading to consistency issues.</li>
              <li><b>Increased Complexity:</b> Implementing caching introduces additional components to monitor and maintain.</li>
              <li><b>Memory Overhead:</b> Caching requires extra memory, and improper cache management can lead to excessive memory use.</li>
            </ul>
            <br>

            <h3 class="section-title">Cache Strategies</h3>

            <table>
                <thead>
                  <tr>
                    <th>Cache Strategy</th>
                    <th>Theory</th>
                    <th>Pros</th>
                    <th>Cons</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><b>Cache Aside</b></td>
                    <td>Also known as Lazy Loading, the application checks the cache first. If data is not in the cache (cache miss), the application retrieves it from the database, places it in the cache, and then returns it to the user.</td>
                    <td>
                      <ul>
                        <li>Simple to implement.</li>
                        <li>Only caches requested data, reducing memory usage.</li>
                      </ul>
                    </td>
                    <td>
                      <ul>
                        <li>Risk of stale data since database updates don't immediately update the cache.</li>
                        <li>Cache misses lead to slower initial responses.</li>
                      </ul>
                    </td>
                  </tr>
                  <tr>
                    <td><b>Read-Through Cache</b></td>
                    <td>The cache sits in front of the database. When a cache miss occurs, the cache fetches the data from the database and updates itself before returning the data to the application.</td>
                    <td>
                      <ul>
                        <li>Application transparency, as read logic is handled by the cache.</li>
                        <li>Fewer cache misses because the cache automatically loads data when missing.</li>
                      </ul>
                    </td>
                    <td>
                      <ul>
                        <li>More complex to implement than Cache Aside.</li>
                        <li>Similar stale data issues if updates arenâ€™t synchronized.</li>
                      </ul>
                    </td>
                  </tr>
                  <tr>
                    <td><b>Write-Through Cache</b></td>
                    <td>Data is written to both the cache and the database simultaneously. Every update first modifies the cache, which writes to the database immediately after.</td>
                    <td>
                      <ul>
                        <li>Ensures consistency between cache and database.</li>
                        <li>No risk of stale data as updates happen in real-time.</li>
                      </ul>
                    </td>
                    <td>
                      <ul>
                        <li>Higher write latency, as both cache and database must be updated.</li>
                        <li>More complexity and overhead due to duplicate writes.</li>
                      </ul>
                    </td>
                  </tr>
                  <tr>
                    <td><b>Write-Behind (Write-Back) Cache</b></td>
                    <td>Data is first written to the cache, and the write to the database happens asynchronously (after a delay).</td>
                    <td>
                      <ul>
                        <li>Improved write performance as the database doesn't need immediate updating.</li>
                        <li>Supports batch writes to the database, reducing total write operations.</li>
                      </ul>
                    </td>
                    <td>
                      <ul>
                        <li>Risk of data loss if the cache fails before the database is updated.</li>
                        <li>More complex logic to manage the asynchronous writes and queues.</li>
                      </ul>
                    </td>
                  </tr>
                </tbody>
            </table>
            <br>
            <h3 class="section-title">Eviction in Cache</h3>
                <p>Eviction refers to the process of removing data from the cache when it reaches its capacity. When new data needs to be added to a full cache, the cache decides which existing data should be evicted based on the chosen eviction policy.</p>
                
                <h5>Eviction Policies</h5>

                <table>
                  <thead>
                    <tr>
                      <th>Policy</th>
                      <th>Theory</th>
                      <th>Pros</th>
                      <th>Cons</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td><b>Least Recently Used (LRU)</b></td>
                      <td>LRU removes the least recently accessed data from the cache when new data is added.</td>
                      <td>Effective for workloads where recently accessed data is more likely to be accessed again.</td>
                      <td>May not perform well in cases where older data is more valuable than recently accessed data.</td>
                    </tr>
                    <tr>
                      <td><b>Least Frequently Used (LFU)</b></td>
                      <td>LFU removes the data that is accessed the least frequently over time.</td>
                      <td>Effective for workloads where some data is consistently more important than others, based on frequency of access.</td>
                      <td>Can lead to stale data if frequently accessed data is no longer needed but remains in the cache.</td>
                    </tr>
                  </tbody>
                </table>
                <br>

            <h3 class="section-title">What is a Distributed Cache?</h3>
                <p>A <b>distributed cache</b> is a cache that is spread across multiple servers or nodes, allowing data to be cached in a decentralized way. This type of cache is designed to scale horizontally, enabling high availability, fault tolerance, and improved performance across large systems.</p>
                <p>Unlike a local cache that is stored on a single server, a distributed cache allows caching across multiple servers, which is crucial for applications serving millions of users or handling significant traffic. Distributed caching ensures data is accessible even when individual servers fail, making it ideal for cloud-native applications and large-scale systems.</p>
                <img src="../../images/distributed_cache_design.PNG" alt="lb" style="display: block; margin: auto; width: 50%;">
            <br>
            <h3 class="section-title">Why Use a Distributed Cache?</h3>
                <ul>
                    <li><b>Scalability:</b> A distributed cache allows the system to handle more data and traffic by adding more nodes (servers) as demand grows, which ensures horizontal scalability.</li>
                    <li><b>Fault Tolerance:</b> If one node in the cache fails, others can take over without causing data loss or downtime. This improves the system's availability and reliability.</li>
                    <li><b>Performance:</b> By distributing cache across multiple nodes, you can reduce the load on any single database or node and speed up data access for different regions or zones, leading to better response times.</li>
                    <li><b>Reduced Database Load:</b> Offloads a significant amount of read and write operations from the primary database by storing frequently accessed data in the cache.</li>
                </ul>
            <br>
            <h3 class="section-title">How to Design a Distributed Cache?</h3>
                <p>Designing a distributed cache involves several key considerations to ensure it is scalable, performant, and fault-tolerant. Below are the essential design steps:</p>

                <h5>1. Data Partitioning (Sharding)</h5>
                <p>To distribute data across multiple nodes, we partition (or shard) the data. A consistent hashing algorithm is often used to determine which node stores a specific piece of data. This ensures that data is distributed evenly across all nodes and can easily be retrieved.</p>
                <ul>
                <li><b>Hashing:</b> The cache keys are hashed, and the resulting hash value determines the node where the data will be stored.</li>
                <li><b>Dynamic Sharding:</b> In some cases, nodes can be dynamically added or removed, and the data is rebalanced automatically.</li>
                </ul>
                <hr>
                <h5>2. Cache Replication</h5>
                <p>Replication ensures that multiple copies of the cached data are stored on different nodes. This improves fault tolerance and availability, as data can still be accessed if a node fails. There are two common replication methods:</p>
                <ul>
                <li><b>Master-Slave Replication:</b> One node holds the master copy of the data, while replicas hold copies for failover purposes.</li>
                <li><b>Peer-to-Peer Replication:</b> Each node holds a replica of the cached data, distributing the load more evenly across the system.</li>
                </ul>
                <hr>
                <h5>3. Consistency Models</h5>
                <p>In a distributed cache, ensuring data consistency across nodes is critical. There are different consistency models that can be employed:</p>
                <ul>
                <li><b>Strong Consistency:</b> Ensures that the most recent write is visible across all nodes before any read operation can return data. This guarantees that all clients see the same data.</li>
                <li><b>Eventual Consistency:</b> Data updates may not be immediately visible across all nodes, but eventually, all nodes will have the same data after a period of time. This is more performant but can lead to temporary inconsistencies.</li>
                </ul>
                <hr>
                <h5>4. Cache Eviction Policies</h5>
                <p>In a distributed environment, cache size is still finite, so eviction policies decide which data should be removed when space is needed for new data. Common eviction policies include:</p>
                <ul>
                <li><b>Least Recently Used (LRU):</b> Removes the least recently accessed data first.</li>
                <li><b>Least Frequently Used (LFU):</b> Removes the data that has been accessed the least over time.</li>
                </ul>
                <hr>
                <h5>5. Handling Cache Invalidation</h5>
                <p>Cache invalidation ensures that stale data is removed or updated to maintain data integrity. In a distributed cache, this is more challenging due to the number of nodes involved. There are several strategies for handling cache invalidation:</p>
                <ul>
                <li><b>Time-to-Live (TTL):</b> Each cached object has an expiration time, after which it is automatically invalidated.</li>
                <li><b>Write Invalidate:</b> The cache is updated when data is written to the database, ensuring the cache is always up-to-date.</li>
                </ul>
                <hr>
                <h5>6. Data Serialization</h5>
                <p>To efficiently store and transfer data across nodes in a distributed cache, it is serialized (converted to a format that can be transmitted or stored). Common serialization formats include:</p>
                <ul>
                <li><b>JSON:</b> Lightweight and human-readable format, but not as performant as binary serialization.</li>
                <li><b>Protobuf:</b> Google's Protocol Buffers offer a compact, fast, and efficient binary serialization format.</li>
                </ul>
                <hr>
                <h5>7. Network Latency and Data Locality</h5>
                <p>In a distributed cache, minimizing network latency is essential to ensuring fast data retrieval. You can design the system to ensure that the cache nodes are geographically distributed and closer to users (data locality) to reduce latency.</p>
                
              </br>
              </div>

            <h3 class="section-title">Popular Distributed Cache Solutions in Cloud Environments</h3>
                <div class="example">
                <ol>
                    <li>Amazon ElastiCache (AWS)</li>
                    <li>Azure Cache for Redis (Azure)</li>
                    <li>Google Cloud Memorystore (GCP)</li>
                    <li>Redis</li>
                    <li>Memcached</li>
                </ol>
                </div>
            
                <br>
            <h3 class="section-title">Interview Questions and Answers</h3>
            <div class="interview-question">
                <strong>Q1: What are the differences between a distributed cache and a CDN (Content Delivery Network)?</strong>
            </div>
            <div class="interview-answer">
                <p>A distributed cache and a CDN both aim to improve data access performance, but they serve different use cases:
                    <h6>Distributed cache:</h6>
                    <ul>
                        <li>Typically used to store frequently accessed data in-memory to reduce load on databases or APIs.</li>
                        <li>Optimized for low-latency access to structured data like database query results, session data, or user-specific information.</li>
                        <li>Often deployed closer to the application (backend systems).</li>
                    </ul>
                    
                    <h6>CDN:</h6>
                    <ul>
                        <li>Primarily used to store and serve static assets (like images, CSS, and JavaScript files) to users globally from locations closer to them.</li>
                        <li>Focuses on improving the delivery of static content and media files by caching data at the edge.</li>
                        <li>Often deployed closer to the end-users geographically.</li>                    
                    </ul>
                    
                </p>
            </div>
</div>
